{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤” Model Selection & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/LP6sUuZ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the full list of metrics [here](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mainly four categories are available: classification metrics, regression metrics, clustering metrics and distance/pairwise metrics\n",
    "\n",
    "- Most popular classification metrics are accuracy, f1_score, precision, recall and confusion matrix (also auc and balanced accuracy) which are provided along with more niche versions\n",
    "\n",
    "- Consider binary classification over `A` and `B` where `A` is the class you are interested in (i.e., positive class). \n",
    "\n",
    "There are four cases when the model classifies a point $x$:\n",
    "\n",
    "- Model classification is correct (agree with true label)\n",
    "    - The true label is A â†’ True Positive (TP)\n",
    "    - The true label is B â†’ True Negative (TN)\n",
    "- Model classification is incorrect (disagrees with the true label)\n",
    "    - The true label is A â†’ False Negative (FN)\n",
    "    - The true label is B â†’ False Positive (FP)\n",
    "\n",
    "- Confusion Matrix (Predicted VS. Actual) Shows all four:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a1/ConfusionMatrixRedBlue.png\" width=300>\n",
    "\n",
    "#### ðŸ§  Thinking of a New Metric\n",
    "- Consider 1M cells as dataset\n",
    "    - 100 have a disease we are interested in predicting\n",
    "    - Rest does not\n",
    "    - A classifier that always classifies \"No Disease\" will be 99.99% correct\n",
    "\n",
    "- Instead of accuracy, divide true positives over the number of positive labels â†’ Recall\n",
    "    - Solves the problem above\n",
    "    - Problem is that model may not always classify as \"Disease\" and will be correct\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "- Instead of accuracy, divide the true positives by the number of positive outputs â†’ Precision\n",
    "    - Solves the problem above\n",
    "    - Problem is that if the model produces 1 positive output then precision is 1\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "- Final Solution\n",
    "    - Make a computation that closely emphasizes the worse of precision and recall\n",
    "    - Call it F1-Score\n",
    "\n",
    "$$ F_1 = \\frac{2}{\\frac{1}{\\text{recall}} + \\frac{1}{\\text{precision}}}$$\n",
    "\n",
    "- In multiclass scenarios, each class will have its own precision and recall (and F1). The final metric can be a normal average (macro) or a weighted average or pooling of confusion matrices (micro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Load Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "x_data = breast_cancer.data\n",
    "y_data = breast_cancer.target\n",
    "\n",
    "# Split the data into training and valing sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Train and Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, Recall, F1 Score: 0.956140350877193, 0.9342105263157895, 1.0, 0.9659863945578231\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier (K-Nearest Neighbors in this example)\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred,)\n",
    "recall = recall_score(y_val, y_pred,)\n",
    "f1 = f1_score(y_val, y_pred,)\n",
    "\n",
    "print(f\"Accuracy, Precision, Recall, F1 Score: {accuracy}, {precision}, {recall}, {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better than this is classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       1.00      0.88      0.94        43\n",
      "      benign       0.93      1.00      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.94      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_val, y_pred, target_names=breast_cancer.target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  5]\n",
      " [ 0 71]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics` has also a small number of plotting functions, and one of them can plot the confusion matrix but it's quite meh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, common regression metrics include L2 loss $L_{2} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ and L1 loss $L_{1} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Load Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target     # disease pregression (continuous)\n",
    "\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Calculate Some Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Mean Absolute Error: 42.794094679599944\n",
      "Linear Regression - Mean Squared Error: 2900.193628493483\n"
     ]
    }
   ],
   "source": [
    "# Fit linear regression model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = linear_reg.predict(X_test)\n",
    "\n",
    "# Calculate L1 and L2 loss using sklearn.metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Linear Regression - Mean Absolute Error:\", mae)\n",
    "print(\"Linear Regression - Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more interpretable metric is $R^2$ score.\n",
    "$$\n",
    "R^2 = 1 - \\frac{{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}}{{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "It measures the improvement of your model over the most naive regressor which just predicts the average of the target. That is, if your $R^2$ is 0.7 then you have reduced the errors done by the naive regressor by $70\\%$ compared to an ideal regressor with zero error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45260276297191915"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we change the random state of the `train-test-split`\n",
    "    - The validation set will change\n",
    "    - The score will likely change\n",
    "    - Our metric estimate (e.g., accuracy) is hence weak\n",
    "\n",
    "- Intuitively, we can get a stronger estimate by trying multiple `train-test-splits` and then averaging the scores together\n",
    "    - This should be closer to the true metric (across the whole population) than one point estimate\n",
    "    - Implies better model selection decisions\n",
    "    - This is the idea of cross-validation where `k` represents the number of folds (i.e., `1/k` is the train-test ratio)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img width=300 src=\"https://www.researchgate.net/publication/368622723/figure/fig1/AS:11431281120962096@1676727198009/5-Fold-iteration-cross-validation.png\">\n",
    "</div>\n",
    "\n",
    "This also gives us a way to evaluate the confidence over the scores (e.g., if the variance between them is small then we can trust that an estimate on a new test set will be close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, KFold, RepeatedKFold, RepeatedStratifiedKFold, LeaveOneOut\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "x_data, y_data = iris.data, iris.target\n",
    "\n",
    "# SVM model\n",
    "svm_model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it. No fit and no split because it's all handled within cross validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only when you are done with model selection train on all `x_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple k-fold cross-validation scores: [1.         1.         0.96666667 0.93333333 0.96666667]\n",
      "Mean accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup the folds\n",
    "kfold_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# 2. Perform cross-validation\n",
    "kfold_cv_scores = cross_val_score(svm_model, x_data, y_data, cv=kfold_cv, scoring='f1_micro')\n",
    "print(\"\\nSimple k-fold cross-validation scores:\", kfold_cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(kfold_cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repeated K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the estimate gets stronger if we repeat the cross-validation multiple times after random shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated cross-validation scores: [1.         1.         0.96666667 0.93333333 0.96666667 1.\n",
      " 1.         0.96666667 1.         0.96666667]\n",
      "Mean accuracy: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "repeated_cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "repeated_cv_scores = cross_val_score(svm_model, x_data, y_data, cv=repeated_cv)\n",
    "print(\"Repeated cross-validation scores:\", repeated_cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(repeated_cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When data has imbalance stratified splits are better (but slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified cross-validation scores: [1.         1.         0.93333333 1.         1.         0.93333333\n",
      " 1.         0.96666667 1.         0.96666667]\n",
      "Mean accuracy: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "stratified_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "stratified_cv_scores = cross_val_score(svm_model, x_data, y_data, cv=stratified_cv)\n",
    "print(\"\\nStratified cross-validation scores:\", stratified_cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(stratified_cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some argue that setting K=M can be even more accurate than standard cross validation (leave-one-out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's a moot point and it takes lots of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Leave-one-out cross-validation scores: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Mean accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "leave_one_out_cv = LeaveOneOut()\n",
    "leave_one_out_cv_scores = cross_val_score(svm_model, x_data, y_data, cv=leave_one_out_cv)\n",
    "print(\"\\nLeave-one-out cross-validation scores:\", leave_one_out_cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(leave_one_out_cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
