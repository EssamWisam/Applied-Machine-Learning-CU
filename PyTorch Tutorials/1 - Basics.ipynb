{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔮 Deep Learning Frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\text{In Supervised Machine Learning:}$\n",
        "\n",
        "- We have a dataset $(x,y)$ representing the true target function $f_t(x)$ (i.e., $y=f_t(x)$)\n",
        "\n",
        "- Any machine learning model is equivalent to some hypothesis set represented by mathematical function $f(x;θ)$\n",
        "\n",
        "- The objective of training is to learn the optimal value of the parameters $θ$ (i.e., find $θ^*$ such that $f_{θ^*}(x)≈f_t(x)$)\n",
        "\n",
        "- Now use $f_{θ^*}(x)$ on real examples; it's a great approximate of $f_t(x)$\n",
        "\n",
        "Hence, mathematically, supervised machine learning is just function approximation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\text{In Deep Learning :}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same concept exactly applies except that $f(x;θ)$ is a matrix function that may involve tons of matrix multiplications (one for each layer).\n",
        "\n",
        "**Awesome Numpy can handle matrix and nd-array operations quite well.**\n",
        "\n",
        "- Not so fast, because it can only do so on the CPU \n",
        "\n",
        "- If $f(x;θ)$ is has more matrices (deeper network), it will take much slower\n",
        "\n",
        "- Meanwhile, GPUs were designed for graphical processing which as well involves tons of matrix multiplications\n",
        "\n",
        "  - They are composed of many (e.g., 1000s) cores each with much smaller clock rate and a VRAM to store the computations (pixels for the screen) supported by higher memory bandwidth and higher memory clock rate.\n",
        "\n",
        "  - 💡 Idea: use the GPU for the matrix computations (but ignore the screen-related facets)\n",
        "\n",
        "  - 🚨 Problem: Numpy operates its array operations on the GPU\n",
        "\n",
        "  - 💡 Solution: Deep Learning Frameworks provide this and many other features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensors\n",
        "\n",
        "- `PyTorch` array type just like `Numpy` arrays but can run on the GPU\n",
        "\n",
        "- Other distinction from Numpy arrays is that they support differentiation (will see why later)\n",
        "\n",
        " - It follows that most if not all operations we can perform in Numpy have their equivalents in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code in the following is adapted from PyTorch's official tutorial and [this article](https://medium.com/codex/tensor-basics-in-pytorch-252a34288f2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Initializing Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 0],\n",
              "        [0, 0]], dtype=torch.int32)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# from a list\n",
        "z = torch.tensor([[1, 2],[3, 4]]) \n",
        "\n",
        "# from a Numpy array \n",
        "z = torch.tensor(np.array(z))\n",
        "\n",
        "# an empty multi-dimensional (2 * 3 * 2) tensor\n",
        "z = torch.empty(2, 3, 2)\n",
        "\n",
        "# a 1*12 vector\n",
        "z = torch.arange(12)\n",
        "\n",
        "# a random 1*2 matrix \n",
        "z = torch.rand(1, 2)\n",
        "\n",
        "# a random 1*2 matrix drawn from Gaussian distribution \n",
        "z = torch.randn(1, 2)\n",
        "\n",
        "# a zero-filled 1*2 matrix \n",
        "z = torch.zeros(1, 2)\n",
        "\n",
        "# a 1*2 matrix filled with only 1\n",
        "z = torch.ones(1, 2)\n",
        "\n",
        "# Specifying the type of the elements of the tensor\n",
        "z = torch.ones(2, 2, dtype=torch.int)\n",
        "\n",
        "# From another tensor\n",
        "z = torch.zeros_like(z)\n",
        "\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Attributes of a Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Number of parameters: 12\n",
            "Device tensor is stored on: cpu\n",
            "Requires Gradient: False\n",
            "Gradient: None\n"
          ]
        }
      ],
      "source": [
        "Z = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {Z.shape}\")\n",
        "print(f\"Datatype of tensor: {Z.dtype}\")\n",
        "print(f\"Number of parameters: {Z.numel()}\")      \n",
        "print(f\"Device tensor is stored on: {Z.device}\")                   \n",
        "print(f\"Requires Gradient: {Z.requires_grad}\")\n",
        "print(f\"Gradient: {Z.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also mostly just like Numpy in indexing/slicing, masking, element-wise operations, mathematical operations, broadcasting, aggregation, etc. \n",
        "\n",
        "Let's explore some differences:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Reshaping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4513, 0.6858],\n",
            "        [0.3407, 0.5679],\n",
            "        [0.0326, 0.5236],\n",
            "        [0.8209, 0.5186],\n",
            "        [0.2622, 0.8980],\n",
            "        [0.5572, 0.8952]])\n"
          ]
        }
      ],
      "source": [
        "### tensor.view is like tensor.reshape but assumes it can reshape without changing memory layout\n",
        "x = Z.view(12)\n",
        "x = Z.view(6, -1)      \n",
        "print(x)\n",
        "assert torch.allclose(x, Z.reshape(6, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Numpy, it `np.view` can only help change type of input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9873909950256348"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.rand(5, 3)\n",
        "\n",
        "col_0 = x[:, 0]         # like Numpy\n",
        "row_0 = x[0, :]         # like Numpy\n",
        "\n",
        "elem_1_2 = x[1, 2]      # returns a Tensor!\n",
        "elem_1_2.item()         # .item() converts to Python scalar    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`axis` usually called `dim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 3])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1 = torch.rand(5, 3)\n",
        "x2 = x = torch.rand(5, 3)\n",
        "\n",
        "x12 = torch.cat((x1, x2), dim=0)            # also called concatenate in Numpy\n",
        "x12.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alias for doing operations in-place:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([21, 22, 23, 24])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([1, 2, 3, 4])\n",
        "a.add_(20)    #torch.add(a, 20, out=a)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "Count = torch.tensor([1, 2, 3, 4])\n",
        "torch.save(Count, 'Count.pt')                   # np.save('Count.npy', a)\n",
        "Count_revived = torch.load('Count.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Type casting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4.])"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Count.float()           # Count.astype(float) in Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general, you can assume they follow the same syntax and search up quickly when an error occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. GPU Support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z lives on cpu\n",
            "x_gpu lives on cpu and x lives on cpu\n"
          ]
        }
      ],
      "source": [
        "# Creating a PyTorch tensor on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "z = torch.tensor([[1, 2, 3], [4, 5, 6]], device=device)\n",
        "print(\"z lives on\", Z.device)\n",
        "\n",
        "x_gpu = x.to(device)\n",
        "print(\"x_gpu lives on\", x_gpu.device, \"and x lives on\", x.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Conversion to Numpy is possible: if only on CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 0]], dtype=int32)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.cpu().numpy()                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Storing Gradients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider\n",
        "$$Z_{1×1}=Y_{1×1}^2$$\n",
        "where\n",
        "$$Y_{1×1} = X_{1×4}^TW_{4×1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can implement this with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the tensors with actual values\n",
        "x = torch.tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True)\n",
        "w = torch.tensor([1.0, 0.0, -1.0, 2.0], requires_grad=True)\n",
        "\n",
        "# Compute the equations:\n",
        "y = torch.dot(x, w)                 # automatically inherits requires_grad=True\n",
        "z = y ** 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now suppose we want to find $\\frac{\\partial Z}{\\partial W}$ then by chain rule we have:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial Z}{\\partial W} = \\frac{\\partial Z}{\\partial Y} * \\frac{\\partial Y}{\\partial W} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly, \n",
        "$$\\frac{\\partial Z}{\\partial Y}=2Y \\quad \\text{and} \\quad \\frac{\\partial Y}{\\partial W}=X \\quad \\text{thus,} \\quad \\frac{\\partial Z}{\\partial W}=2*Y*W$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([32., 48., 64., 80.], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "მzⳆმw = 2 * y * x\n",
        "მzⳆმw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well, PyTorch can automatically do this for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 16.,   0., -16.,  32.]) tensor([32., 48., 64., 80.])\n"
          ]
        }
      ],
      "source": [
        "z.backward()            # compute მzⳆმ☘️ for each ☘️ (leaf) in z's computational graph (let's draw it)\n",
        "\n",
        "print(x.grad, w.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**As we will see later** this (automatic differentiation) is the most fantastic feature provided by deep learning frameworks to deep learning engineers and researchers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Can be turned off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True)\n",
        "w = torch.tensor([1.0, 0.0, -1.0, 2.0], requires_grad=True)\n",
        "\n",
        "with torch.no_grad():       # sets all of the requires_grad flags to false for operations in the scope\n",
        "    y = torch.dot(x, w)\n",
        "    z = y ** 2\n",
        "\n",
        "# z.backward() error!\n",
        "print(y.requires_grad) \n",
        "print(y.grad)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Sometimes must be turned off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    w_numpy = w.cpu().numpy()\n",
        "\n",
        "# alternatively:\n",
        "w.detach().cpu().numpy()               # Numpy conversion: if it requires gradient need to detach it from the graph first!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We covered in this notebook:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Why GPUs are important for deep learning\n",
        "\n",
        "- How deep learning frameworks (i.e., PyTorch) solved Numpy's lack for such feature\n",
        "\n",
        "- Initialization, Attributes and Operations over PyTorch tensors\n",
        "\n",
        "- GPU and automatic differentiation support!"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
