{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fb4ab1",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network - Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d6f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27cd74",
   "metadata": {},
   "source": [
    "## üç≥ Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9928f45",
   "metadata": {},
   "source": [
    "Let's have a look at the dataset. Our objective is to predict whether a message is ham or spam. An example client that would benefit from this is a communication company that likes to protect its clients of spam messages like `me and your uncle have founds lots of gold and tombs underground and we need...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43074bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will √º b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will √º b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./sms.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d3272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = df['Message'], df['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2cb394",
   "metadata": {},
   "source": [
    "Let's convert the sequence of sentences `x_data` to a sequence of (sequences of words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5785296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/essam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/essam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'got', 'amore', 'wat', '...']  is  ham\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')                          # so that we can ignore them as they are irrelevant (a, the, in, but, for, etc.)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # (1) Splitting texts into words (so later it can be a sequence of vectors)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # (2) Stop word removal\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "# Preprocess the dataset's text data\n",
    "x_data_processed = [preprocess_text(text) for text in x_data]\n",
    "print(x_data_processed[0], \" is \", y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9ab313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary in our dataset is ['PAD', 'Go', 'point', ',', 'crazy', '..', 'Available', 'bugis', 'n']... and has 5199 words\n"
     ]
    }
   ],
   "source": [
    "# Let's map from each word to its frequency\n",
    "all_words = [word for text in x_data_processed for word in text]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Assume all words that occur less than 2 times in the dataset are unimportant and ditch them (to reduce the dimensionality of the input)\n",
    "min_freq = 2\n",
    "vocabulary = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "\n",
    "# Prepend PAD token with index 0\n",
    "vocabulary.insert(0, 'PAD')\n",
    "\n",
    "# Create a dictionary for word -> index mapping (improves lookup speed)\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "\n",
    "print(f\"The vocabulary in our dataset is {list(vocabulary)[:9]}... and has {len(vocabulary)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the text data by mapping each word to its index in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "672b316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    [word_to_index[word] for word in sublist if word in word_to_index]\n",
    "    for sublist in x_data_processed\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffcc25",
   "metadata": {},
   "source": [
    "Now let's pad or truncate the sequences so they're all of the same length of `max_seq_length` (as the batch must be a tensor which can't store vectors of different sizes). We know that `0` corresponds to the PAD token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088aa364",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "padded_sequences = [[0] * max(0, max_seq_length - len(seq)) +seq[:max_seq_length] for seq in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7ac4d",
   "metadata": {},
   "source": [
    "Setup train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c059404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 0, 'spam': 1}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "category_to_label = {category: label for label, category in enumerate(set(y_data))}\n",
    "print(category_to_label)\n",
    "encoded_labels = [category_to_label[category] for category in y_data]\n",
    "\n",
    "# Assuming you have already loaded your data into X_processed and encoded_labels\n",
    "x_data_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "y_data_tensor = torch.tensor(encoded_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(x_data_tensor, y_data_tensor)\n",
    "train_dataset, val_dataset = random_split(dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995e70c",
   "metadata": {},
   "source": [
    "## üö¢ Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8332a312",
   "metadata": {},
   "source": [
    "A recurrent neural network is just like a feedforward neural network with recurrent layers. The recurrent layer:\n",
    "- Assumes that the input is a sequence of vectors and not a single vector\n",
    "\n",
    "- Outputs a vector for each of those in the sequence via a linear transformation followed by a non-linearity\n",
    "\n",
    "- It shares information between these vectors by involving the output of the layer from the previous time step to the compuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390d101",
   "metadata": {},
   "source": [
    "The following example compares a feedforward neural network with two feedforward layers to a recurrent neural network with a recurrent layer and a feedforward layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186a546",
   "metadata": {},
   "source": [
    "<img width=\"1300\" src=\"https://i.imgur.com/DGSeJpG.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198030c",
   "metadata": {},
   "source": [
    "This is the general case where we may be interested in classifying each token in the sequence. In this notebook, we classify a less general case where we classify the whole sentence into one of two classes (spam or ham). In this case, we only need the final hidden state (which carries information about all previous ones) to make the classification by passing it to the final layer (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff63fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Behaves like a linear layer but with no bias (i.e., z = W @ x)\n",
    "        # It converts the sequence of integers (representing a sequence of one-hot vectors) corresponding to a sentence x\n",
    "        # To a sequence of vectors by applying W @ x for each one-hot vector x. More disucssion goes into why bias is dropped.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)          \n",
    "\n",
    "        # This is the RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, nonlinearity='tanh', batch_first=True)      \n",
    "        # (batch_size, seq_len, H_in) -> (batch_size, seq_len, H_out)\n",
    "        \n",
    "        # This is a linear layer to classify the final output of the RNN\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # An embedding layer that does Wx for each one-hot vector x represented as an integer\n",
    "        embedded = self.embedding(x)\n",
    "                \n",
    "        # Get RNN output (last hidden state)\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        rnn_output = rnn_output[:, -1, :]    # We are only interested in the final hidden state (carries information for all timesteps)\n",
    "        \n",
    "        # Get the classification out of the last hidden state\n",
    "        output = self.fc2(rnn_output)\n",
    "        return output\n",
    "\n",
    "# Assuming vocab_size, embedding_dim, hidden_dim, and output_dim are defined\n",
    "vocab_size = len(vocabulary)\n",
    "model = RNNModel(vocab_size, embedding_dim=64, hidden_dim=64, output_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4916715",
   "metadata": {},
   "source": [
    "Other deep sequence models that extend RNNs to solve some of its issues during training are LSTMs and GRUs. Read more about them [here](https://medium.com/towards-data-science/an-intuitive-approach-to-understading-of-lstms-and-grus-c2191611a37d). Try to increase the accuracy using them later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34c6b5",
   "metadata": {},
   "source": [
    "## üí™ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e2b8296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af16716bd964cb2aed90118fa783d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()           # In the loss function give \"spam\" examples higher weight\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,)\n",
    "\n",
    "num_epochs = 5\n",
    "num_batches = len(train_loader)\n",
    "with tqdm(range(num_epochs), desc=\"Epochs\") as epoch_bar:\n",
    "    for epoch in epoch_bar:\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss, num_correct = 0.0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            # 1. Forward pass\n",
    "            yÃÇb = model(xb)\n",
    "            \n",
    "            # 2. Backward pass\n",
    "            loss = criterion(yÃÇb, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 3. Optimization step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 4. Statistics:\n",
    "            yÃÇb = torch.argmax(yÃÇb, dim=1)\n",
    "            num_correct += (yÃÇb == yb).sum().item()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        accuracy = num_correct / (num_batches * train_loader.batch_size)\n",
    "        epoch_bar.set_postfix(loss=average_loss, accuracy = accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffe6cc",
   "metadata": {},
   "source": [
    "## üïµÔ∏è Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5524785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9723\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(val_loader)\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    num_correct = 0\n",
    "    for xb, yb in val_loader:\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        yÃÇb = model(xb)\n",
    "        \n",
    "        # 2. Statistics \n",
    "        yÃÇb = torch.argmax(yÃÇb, dim=1)\n",
    "        num_correct += (yÃÇb == yb).sum().item()\n",
    "        \n",
    "    accuracy = num_correct / (num_batches * val_loader.batch_size)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bd5e0",
   "metadata": {},
   "source": [
    "### üíª Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37329153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def predict_single_example(example_text, model, category_to_label):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 1. Preprocess the example text\n",
    "    tokens = word_tokenize(example_text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 2. Convert text to sequence using tokenizer\n",
    "    sequence = [vocabulary.index(word) for word in filtered_tokens if word in vocabulary]\n",
    "    padded_sequence = [0] * max(0, max_seq_length - len(sequence)) +sequence[:max_seq_length]\n",
    "    \n",
    "    # 3. Convert padded sequence to tensor\n",
    "    input_tensor = torch.tensor(padded_sequence, dtype=torch.long)\n",
    "    \n",
    "    # 4. Add batch dimension\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "    \n",
    "    # 5. Make prediction\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    # Map predicted label to category\n",
    "    predicted_category = \"Spam\" if predicted_label == 1 else \"Not Spam\"\n",
    "    \n",
    "    return predicted_category, probabilities.numpy()[0][predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8458df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Spam, Confidence: 0.9985\n"
     ]
    }
   ],
   "source": [
    "# choose a random sentence form val_dataset\n",
    "example_text = \"URGENT! You have won a 1 week FREE membership in our ¬£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\"\n",
    "\n",
    "predicted_category, confidence = predict_single_example(example_text, model, category_to_label)\n",
    "print(f\"Predicted Category: {predicted_category}, Confidence: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c57887b",
   "metadata": {},
   "source": [
    "**Note**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa8d2e",
   "metadata": {},
   "source": [
    "We didn't use [torchtext](https://github.com/pytorch/text) because PyTorch will no longer develop/support this package. It has indeed been overtaken by [HuggingFace](https://github.com/huggingface) in the industry."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2050,
     "sourceId": 3494,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.215595,
   "end_time": "2024-04-17T14:27:40.675934",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T14:27:08.460339",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
