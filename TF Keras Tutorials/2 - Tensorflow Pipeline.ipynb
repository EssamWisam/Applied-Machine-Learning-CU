{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Deep Learning in Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/wjTxjxS.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have Considered TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only as a tensor library (i.e., like Numpy) but which also offers GPU support and automatic differentiation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/mFrR2eg.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr> <th>tf</th> <th>tf.data</th> <th>tf.keras</th> <th>Deployment</th> </tr>\n",
    "  <tr> \n",
    "  <td> Wraps all other modules and offers tensor functionality on GPU and automatic differentiation </td> \n",
    "  <td> Functionality related to reading and handling data </td> \n",
    "  <td> Mainly exports layers, losses, etc. (similar to torch.nn) but actually does much more than that </td> \n",
    "  <td> TensorFlow.js to train and deploy models in JavaScript, TFLite to deploy on edge devices (mobile) and more deployment friendly stuff</td> \n",
    "</table>\n",
    "\n",
    "Notice that unlike PyTorch:\n",
    "- There is much better support for deployment on different platforms\n",
    "- TensorFlow is more popular than PyTorch\n",
    "- Large part of the high-level functionality is delegated to `tf.keras` (and we will see later that `keras` can operate completely independently from tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Reading and Handling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like PyTorch, we wrap our data in a Dataset object. \n",
    "\n",
    "Usually you will use `tf.data.Dataset.from_tensor_slices`, what it does is take a tensor (or something that could be converted to that) of shape `(N1,N2,...)` and splits it into `N1` tensors of shape `(N2,N3,...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(2,), dtype=tf.int32, name=None)\n",
      "It's an instance of Dataset: True\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([5 6], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4], [5, 6]]) # not indexable but iterable.\n",
    "print(dataset.element_spec)                                            # can't use shape  on whole dataset but this gives shape of one element\n",
    "\n",
    "print(f\"It's an instance of Dataset: {isinstance(dataset, tf.data.Dataset)}\")\n",
    "for element in dataset: print(element)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some basic operations we can do with our dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([25 36], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 9 16], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# filtering\n",
    "filtered_dataset = dataset.filter(lambda x : x[0] > 2)\n",
    "# mapping\n",
    "mapped_dataset = dataset.map(lambda x : x**2)\n",
    "# shuffling \n",
    "shuffled_dataset = mapped_dataset.shuffle(buffer_size=len(mapped_dataset))\n",
    "for element in shuffled_dataset: print(element)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipping is also possible (and can be handled with `from_tensor_slices` as we will see later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: [1. 2.] Label: 0\n",
      "Feature: [3. 4.] Label: 1\n",
      "Feature: [5. 6.] Label: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Dummy features and labels\n",
    "x_data = tf.data.Dataset.from_tensor_slices([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "y_data = tf.data.Dataset.from_tensor_slices([0, 1, 0])  # Binary classification labels\n",
    "\n",
    "# Combine features and labels\n",
    "dataset = tf.data.Dataset.zip((x_data, y_data))\n",
    "\n",
    "# Iterate over the combined dataset\n",
    "for x, y in dataset:\n",
    "    print(\"Feature:\", x.numpy(), \"Label:\", y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of Features: [[1. 2.]\n",
      " [3. 4.]]\n",
      "Batch of Labels: [0 1]\n",
      "It's an instance of Dataset: True\n"
     ]
    }
   ],
   "source": [
    "# Batch the dataset\n",
    "batch_size = 2\n",
    "batched_dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)        \n",
    "# prefetch \"When the GPU is propagating on the current batch\n",
    "# we want the CPU to process the next batch of data so that it is immediately ready.\"\n",
    "\n",
    "# Iterate over the batched dataset\n",
    "for xb, yb in batched_dataset:\n",
    "    print(\"Batch of Features:\", xb.numpy())\n",
    "    print(\"Batch of Labels:\", yb.numpy())\n",
    "\n",
    "# Unlike PyTorch, no independent concept of Dataloader:\n",
    "print(f\"It's an instance of Dataset: {isinstance(batched_dataset, tf.data.Dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting is possible but requires more manual work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 5\n",
      "Training batches: 4\n",
      "Testing batches: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example dataset\n",
    "x_data, y_data = tf.random.uniform([10, 5, 5]), tf.random.uniform([10, 1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "\n",
    "# Batch the dataset\n",
    "batch_size = 2\n",
    "batched_dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Determine the number of batches for training and testing\n",
    "num_batches = len(list(batched_dataset))\n",
    "train_size = int(0.8 * num_batches)\n",
    "test_size = num_batches - train_size\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_dataset = batched_dataset.take(train_size)            # take the first train_size elements (batches)\n",
    "test_dataset = batched_dataset.skip(train_size)             # skip the first train_size elements then take rest\n",
    "\n",
    "# Verify the split\n",
    "print(f\"Total batches: {num_batches}\")\n",
    "print(f\"Training batches: {len(list(train_dataset))}\")\n",
    "print(f\"Testing batches: {len(list(test_dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we will at most need for this tutorial but you can dive deeper in the following official [TF tutorial](https://www.tensorflow.org/guide/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/lEo6rtk.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shift our focus to Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/SbcHrMK.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen it looks like a standalone deep learning library. The reality is that it uses TensorFlow under the hood to implement different functionalities (e.g., layers, losses, optimizers, etc.) just like how `torch.nn` uses `torch` and its other modules to implement the same constructs.\n",
    "\n",
    "To be precise, this fact has been true for so long, but as we will see at the end of the tutorial, `Keras 3` (most recent release) goes beyond only supporting such constructs for TensorFlow (but for now let's ignore this fact)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
