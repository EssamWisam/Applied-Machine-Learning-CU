{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Deep Learning Frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another well known deep learning framework is Tensorflow by Google.\n",
        "\n",
        "- PyTorch has a more Pythonic syntax (i.e., feels more natural)\n",
        "\n",
        "- Has that edge over TensorFlow as well as supporting a dynamic computational graph (which was recently added to TF)\n",
        "\n",
        "  - I.e., can modify the computation graph during runtime without incurring a performance penalty\n",
        "\n",
        "- PyTorch overall [more popular](https://trends.google.com/trends/explore?date=today%205-y&q=%2Fg%2F11gd3905v1,Tensorflow&hl=en) and is widely preferred by researchers \n",
        "\n",
        "- That said, TensorFlow has a longer history of success with deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Note we assume in the following tutorials that you have went over the PyTorch version (we will draw parallels/differences between them)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensors\n",
        "\n",
        "- `Tensor` array type just like `Numpy` arrays but can run on the GPU\n",
        "\n",
        "- Other distinction from Numpy arrays is that they support differentiation (will see why later)\n",
        "\n",
        " - It follows that most if not all operations we can perform in Numpy have their equivalents in Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code in the following is adapted from PyTorch's official tutorial and [this article](https://medium.com/codex/tensor-basics-in-pytorch-252a34288f2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Initializing Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0 0]\n",
            " [0 0]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# from a list\n",
        "z = tf.Variable([[1, 2],[3, 4]])        # Gradients can be tracked for this (but technically lacks Tensor type)\n",
        "z = tf.constant([[1, 2],[3, 4]])        # But not this and both of these don't support element-wise assignment\n",
        "\n",
        "# from a Numpy array \n",
        "z = tf.constant(np.array(z))            # Can alternatively wrap in a variable\n",
        "\n",
        "# an empty multi-dimensional (2 * 3 * 2) tensor\n",
        "z = tf.zeros((2, 3, 2))\n",
        "\n",
        "# a 1*12 vector\n",
        "z = tf.range(12)\n",
        "\n",
        "# a random 1*2 matrix \n",
        "z = tf.random.uniform((1, 2))\n",
        "\n",
        "# a random 1*2 matrix drawn from Gaussian distribution \n",
        "z = tf.random.normal((1, 2))\n",
        "\n",
        "# a zero-filled 1*2 matrix \n",
        "z = tf.zeros((1, 2))\n",
        "\n",
        "# a 1*2 matrix filled with only 1\n",
        "z = tf.ones((1, 2))\n",
        "\n",
        "# Specifying the type of the elements of the tensor\n",
        "z = tf.ones((2, 2), dtype=tf.int32)\n",
        "\n",
        "# From another tensor\n",
        "z = tf.zeros_like(z)\n",
        "\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Attributes of a Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: (2, 2)\n",
            "Datatype of tensor: <dtype: 'int32'>\n",
            "Device tensor is stored on: /job:localhost/replica:0/task:0/device:CPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Creating a sample tensor\n",
        "Z = tf.constant([[1, 2], [3, 4]])       \n",
        "\n",
        "# Shape of tensor\n",
        "print(f\"Shape of tensor: {Z.shape}\")\n",
        "\n",
        "# Datatype of tensor\n",
        "print(f\"Datatype of tensor: {Z.dtype}\")\n",
        "\n",
        "# Device tensor is stored on\n",
        "print(f\"Device tensor is stored on: {Z.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also mostly just like Numpy in indexing/slicing, masking, element-wise operations, mathematical operations, broadcasting, aggregation, etc. \n",
        "\n",
        "There will be some difference but as we argued they will be easy to get around."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[1 2 3 4 5 6 7 8 9]], shape=(1, 9), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor([2 5 8], shape=(3,), dtype=int32)\n",
            "tf.Tensor([6 7 8 9], shape=(4,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 3  4  5]\n",
            " [ 7  8  9]\n",
            " [11 12 13]], shape=(3, 3), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 2  4  6]\n",
            " [12 15 18]\n",
            " [28 32 36]], shape=(3, 3), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 3  4  5]\n",
            " [ 6  7  8]\n",
            " [ 9 10 11]], shape=(3, 3), dtype=int32)\n",
            "tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant([[1, 2, 3],\n",
        "                 [4, 5, 6],\n",
        "                 [7, 8, 9]])\n",
        "\n",
        "# reshape\n",
        "y = tf.reshape(x, (1, -1))\n",
        "print(y)\n",
        "\n",
        "# slicing\n",
        "print(x[0,1])                   # no .item() like PyTorch\n",
        "print(x[:, 1])\n",
        "\n",
        "# masking\n",
        "mask = x > 5\n",
        "print(x[mask])\n",
        "\n",
        "# element-wise oeprations\n",
        "y = tf.constant([[2, 2, 2],\n",
        "                 [3, 3, 3],\n",
        "                 [4, 4, 4]])\n",
        "print(x + y)\n",
        "print(x * y)\n",
        "\n",
        "# broadcasting\n",
        "scalar = tf.constant(2)\n",
        "print(x + scalar)\n",
        "\n",
        "# aggregation\n",
        "print(tf.reduce_mean(x, axis=0))                # it kept the \"axis\" term but changed whole name..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TF is trying to extend support to Numpy operations via `import tensorflow.experimental.numpy as tnp`. See more [here](https://www.tensorflow.org/guide/tf_numpy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. GPU Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike PyTorch, in Tensorflow tensors will default to the GPU whenever possible (i.e., operations supports it). However, manual device placement is also possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a lives on /job:localhost/replica:0/task:0/device:CPU:0\n",
            "c lives on /job:localhost/replica:0/task:0/device:CPU:0\n"
          ]
        }
      ],
      "source": [
        "with tf.device('/CPU:0'):               # Meanwhile, /GPU:0 is your first GPU (or only one).\n",
        "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "  print(\"a lives on\", a.device)\n",
        "\n",
        "# The operation supports GPU and is outside the with scope ‚Üí move the tensors to GPU then perform the operation \n",
        "c = tf.matmul(a, b)\n",
        "print(\"c lives on\", c.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Conversion to Numpy is possible and automatically takes it to CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 0]], dtype=int32)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.numpy()                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Storing Gradients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider\n",
        "$$Z_{1√ó1}=Y_{1√ó1}^2$$\n",
        "where\n",
        "$$Y_{1√ó1} = X_{1√ó4}^TW_{4√ó1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can implement this with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradients: tf.Tensor(16.0, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the tensors with actual values\n",
        "x = tf.constant([2.0, 3.0, 4.0, 5.0], dtype=tf.float32)      # No tracking here\n",
        "w = tf.Variable([1.0, 0.0, -1.0, 2.0], dtype=tf.float32)\n",
        "\n",
        "# Need to explictly define a gradient tape to record the gradients of variables\n",
        "with tf.GradientTape() as tape:\n",
        "    y = tf.tensordot(tf.transpose(x), w, 1)                 # axis=1 is equivalent to matrix multiplication\n",
        "    z = y ** 2\n",
        "\n",
        "# Compute gradients\n",
        "·Éõz‚≥Ü·Éõy = tape.gradient(z, y)                                 # works whether leave or not (let's try y, w, x). what happens for x?\n",
        "\n",
        "print(\"Gradients:\", ·Éõz‚≥Ü·Éõy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We covered in this notebook:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Why GPUs are important for deep learning\n",
        "\n",
        "- How deep learning frameworks (i.e., Tensorflow) solved Numpy's lack for such feature\n",
        "\n",
        "- Initialization, Attributes and Operations over Tensorflow tensors\n",
        "\n",
        "- GPU and automatic differentiation support!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öîÔ∏è PyTorch VS. TensorFlow Revisited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the trend graph above, we see that initially TensorFlow was more successful than PyTorch and that PyTorch afterwards took over:\n",
        "\n",
        "<img src=\"https://i.imgur.com/r11hV5i.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The problem in TensorFlow that made PyTorch much more favorable is that it used a static computational graph back then (and that it's syntax was less friendly). Back then, we would make a graph in TensorFlow as follows:\n",
        "```python\n",
        "# This is TensorFlow 1.x (should be no longer used)\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the graph\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z = x * x * y + y + 2\n",
        "\n",
        "# Compute gradients\n",
        "gradients = tf.gradients(z, [x, y])\n",
        "\n",
        "# Create a session and run the graph\n",
        "with tf.Session() as sess:\n",
        "    # Graph will be optimized and compiled first time this is run and placeholders in graph will be set to 3.0, 4.0\n",
        "    z_value, gradients_value = sess.run([z, gradients], feed_dict={x: 3.0, y: 4.0})\n",
        "    # We passed to sess.run the tensors we want to evaluate from the graph\n",
        "    print(\"Result in TensorFlow 1.0:\", z_value)\n",
        "    print(\"Gradients in TensorFlow 1.0:\", gradients_value)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The problem with this is that since the graph is compiled (hence, static graph): (i), it can't be changed in runtime (which can be useful when input size isn't constant for the model) and (ii), it's so hard to debug which is a big restriction to research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meanwhile, PyTorch utilized a dynamic graph, it doesn't compile it so debugging an error midway through the graph is easy and they can change in runtime. \n",
        "\n",
        "It took TensorFlow long enough to reimplement graphs to support this as well (which is what we covered in this notebook: Tensorflow 2.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf                     # import torch\n",
        "\n",
        "# Just like PyTorch Define the variables\n",
        "x = tf.Variable(3.0)                          # torch.tensor(3.0, requires_grad=True)\n",
        "y = tf.Variable(4.0)                          # torch.tensor(4.0, requires_grad=True)\n",
        "\n",
        "# Perform operations directly\n",
        "with tf.GradientTape() as tape:\n",
        "    z = x * x * y + y + 2\n",
        "\n",
        "# Compute the gradients\n",
        "gradients = tape.gradient(z, [x, y])        # computed in PyTorch with z.backward() then z.grad, y.grad\n",
        "\n",
        "# Get the values\n",
        "z_value = z.numpy()\n",
        "gradients_value = [grad.numpy() for grad in gradients]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But with this TensorFlow is slower (since compiling the graph allows optimizing it first as well and dynamic means that the graph is traced in every single run). For this, TensorFlow added the `@tf.function` which compiles the function into a static graph. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradients(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Perform the computation\n",
        "        z = x * x * y + y + 2\n",
        "    # Compute the gradients\n",
        "    gradients = tape.gradient(z, [x, y])\n",
        "    return z, gradients\n",
        "\n",
        "# Define the variables\n",
        "x = tf.Variable(3.0, dtype=tf.float32)\n",
        "y = tf.Variable(4.0, dtype=tf.float32)\n",
        "\n",
        "# Call the function\n",
        "z, gradients = compute_gradients(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rationally, you should only use it when you are no longer developing the function as you won't be able to debug effectively with it there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analogously, PyTorch also supports a static mode, although it's less often used and a more efficient dynamic implementation may be the reason:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def foo(x, y):\n",
        "    a = torch.sin(x)\n",
        "    b = torch.cos(y)\n",
        "    return a + b\n",
        "\n",
        "optimized_foo = torch.compile(foo)\n",
        "optimized_foo(torch.tensor(0.0),torch.tensor(0.0))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
