{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Deep Learning in Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/0OQVA5H.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of what we have covered applies to Keras 2.x where Keras is considered as a submodule of TensorFlow. Keras 3.0 which has been released just a few months ago has improved in few dimensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qq2BDBT.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's no longer dependent on TensorFlow. It could also be used with PyTorch or Jax!\n",
    "\n",
    "How? Remeber that we argued before that with `torch` or `tensorflow` only, we can build any architecture we want (since layers in `tf.keras.layers` or `torch.nn` are just mathematical operations that can be implemented with the base functionality (tensor processing library) which also includes GPU compuation and automatic differentiation).\n",
    "\n",
    "- Thus, with keras 3.0, when an archiecture is defined, it can be used as a TensorFlow, PyTorch or Keras model by setting an environment varianle `os.environ['backend']`.\n",
    "\n",
    "- If `os.environ['backend'] = 'torch'` then it will use implementations of the layers in the architecture using `torch` and if it's `os.environ['backend'] = 'tensorflow'` then it will use tensorflow implementations and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade keras                # Get Keras 3.0\n",
    "import keras                                # Use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Most if not all what we had before remains the same otherwise, just that lines like `tf.keras.Model` or `tf.keras.layers.Dense` will become `keras.Model` and `keras.layers.Dense` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['backend'] = 'torch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tqdm.keras import TqdmCallback\n",
    "from keras.layers import Dense, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "class NeuralNet(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.model = keras.Sequential([\n",
    "            Dense(256),\n",
    "            LeakyReLU(negative_slope=1e-2),\n",
    "            Dense(128),\n",
    "            LeakyReLU(negative_slope=1e-2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "x_data, y_data = make_moons(n_samples=700, noise=0.1, random_state=42)\n",
    "\n",
    "# make tensor dataset\n",
    "x_data_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "y_data_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "dataset = TensorDataset(x_data_tensor, y_data_tensor)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train Keras Model with PyTorch Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9cff46728d4352bd859fc4c1da64a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7939e8569264bde9372124f8cec8bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29bf0ec70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss=BinaryCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'],\n",
    "            run_eagerly=False\n",
    "            )\n",
    "\n",
    "model.fit(train_loader, epochs=50, verbose=False, callbacks=[TqdmCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0102  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0, 'loss': 0.010292550548911095}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, wrap the dataset in TensorFlow `Dataset` object and use a TensorFlow backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish, we can even use Keras model with custom PyTorch loop (similar to what we did before with TensorFlow). For examples, [see this](https://keras.io/guides/writing_a_custom_training_loop_in_torch/) and [this](https://keras.io/guides/custom_train_step_in_torch/). PyTorch optimizers/losses are even allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î So what's Keras.ops?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It (supposedly) has all [tensor operations](https://keras.io/api/ops/) found in the core of packages such as PyTorch, TensorFlow, Jax. The implementation of the operation is decided based on `os.environ['backend]`.\n",
    "\n",
    "Hence, it helps you write deep learning code that will work for with all deep learning frameworks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, we earlier made the following implementation for a linear layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim=20, output_dim=32):\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, output_dim),trainable=True)\n",
    "        self.b = self.add_weight(shape=(output_dim,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "\n",
    "layer = Linear(20, 32)                               # a layer with 32 neurons; input size will be inferred\n",
    "inputs = tf.random.uniform(shape=(16, 20))          # batch of 16 examples, each of dimensionality 20\n",
    "outputs = layer(inputs)\n",
    "outputs.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer is not generic, if we use it in a model then we can't switch to PyTorch implementation with `os.environ['backend'] = \"torch\"` as specific TensorFlow code is written. To make it generic, we use the generic `matmul` found in `keras.ops`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, input_dim=32, output_dim=20):\n",
    "        super().__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, output_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(output_dim,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return keras.ops.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "layer = Linear(20, 32)                              \n",
    "inputs = torch.rand(16, 20)          # batch of 16 examples, each of dimensionality 20\n",
    "outputs = layer(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we switch to TensorFlow and send a TF tensorflow object, it will still work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['backend'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "layer = Linear(20, 32)                               # a layer with 32 neurons; input size will be inferred\n",
    "inputs = tf.random.uniform(shape=(16, 20))          # batch of 16 examples, each of dimensionality 20\n",
    "outputs = layer(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is fantastic because it allows:**\n",
    "- Using PyTorch via a high-level API (e.g., not writing training loop, using metrics or callbacks, etc.)\n",
    "   - There is an independent package called `PyTorch Lightning` built on top of PyTorch that you can check out. However, it of course doesn't support multiple backends and not a lot of people like it.\n",
    "\n",
    "- Developing in your favorite framework (e.g., PyTorch) then deploying on another framework where it's easier (e.g., TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to this, if a model is saved in the ONNX format (easy to find out how for any framework) then it can (theoretically) be loaded on any other framework, if the operations used in the architecture don't go beyond what ONNX supports.\n",
    "\n",
    "ONNX stands for `Open Neural Network Exchange`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media2.giphy.com/media/26u4lOMA8JKSnL9Uk/giphy.gif?cid=6c09b952mz8j6vf96mg8afz545e2a13zfmmtt46g1r85h6jc&ep=v1_gifs_search&rid=giphy.gif&ct=g\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand TensorFlow and Keras in sufficient depth. Let's revisit the applications we did before with PyTorch but this Keras/TF instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
