{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¢ Multiclass, Multioutput and Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/KwAfLy7.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many models support this inherently (e.g., Bayes classifier)\n",
    "- Many as well don't (e.g., Perceptron)\n",
    "\n",
    "#### ðŸ¤” Problem Statement\n",
    "\n",
    "How do we make a multiclass classifier out of a binary classifier?\n",
    "\n",
    "|                        | One-vs-Rest                           | One-vs-One                        |\n",
    "|------------------------|---------------------------------------|-----------------------------------|\n",
    "| **Idea**               | Trains one classifier per class, treating it as positive and the rest as negative. | Trains a binary classifier for each pair of classes, distinguishing between them. |\n",
    "| **Training**           | Requires training n binary classifiers where n is the number of classes. Each classifier learns to distinguish one class from the rest. | Requires training (n * (n - 1)) / 2 binary classifiers, where n is the number of classes, each trained on data from only two classes. |\n",
    "| **Inference**          | During inference, all classifiers are applied, and the class where the corresponding classifier reported highest score or probability is chosen  | During inference, a voting scheme (e.g., majority voting) is used among all pairwise classifiers to determine the final class label. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn supports both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 2, 0,\n",
       "       0, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "x_data, y_data = load_iris(return_X_y=True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.33, shuffle=True, random_state=0)\n",
    "clf = OneVsOneClassifier(Perceptron(max_iter=1000, eta0=0.1, random_state=0))\n",
    "clf.fit(x_train, y_train)\n",
    "clf.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multioutput Regression or Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn trains a model for each target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "x_data, y_data = make_multilabel_classification(n_classes=3, random_state=0)\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultiOutputClassifier(Perceptron())\n",
    "clf.fit(x_data, y_data)\n",
    "clf.predict(x_data)[0]          # three predictions for the first data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need it for two reasons:\n",
    "- Some classifiers output scores but we are interested in probabilities (and standard normalization can be inaccurate)\n",
    "\n",
    "- Some classifiers tend to output biased probabilities which you some examples for [here](https://scikit-learn.org/stable/modules/calibration.html#calibration-curves)\n",
    "\n",
    "- For instance, Naive Bayes tends to push probabilities to 0 or 1 depending on the degree of violation of its assumption and the number of variables.\n",
    "\n",
    "Calibrating a classifier consists of fitting a regressor (called a calibrator) that maps the output of the classifier $f_i$ (as given by `decision_function` or `predict_proba`) to a calibrated probability in $[0, 1]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_{i}=1|f_{i})=\\frac{1}{1+exp(Af_{i}+B)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressor simply learns $A$ and $B$ given a dataset of the model outputs $f_i$ and the true labels $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "x_data, y_data = make_classification(n_samples=1000, n_features=30, n_redundant=15,  n_repeated=10, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.33, shuffle=True, random_state=0)\n",
    "\n",
    "base_clf = GaussianNB()\n",
    "calibrated_clf = CalibratedClassifierCV(base_clf, method=\"sigmoid\", cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.806060606060606"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_clf.fit(x_train, y_train)\n",
    "base_clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8121212121212121"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibrated_clf.fit(x_train, y_train)\n",
    "calibrated_clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally our machine learning engineer journey is done**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExNmJkcXQ1eDRtbW1ncmh6amppc2dtcGZ2aWFzdTllNHNnMHR6N3E5eSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lD76yTC5zxZPG/giphy.gif\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
